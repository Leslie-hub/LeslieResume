<!DOCTYPE html>
<html lang='en'>

<head>
  <meta charset='UTF-8'>
  <title>Data Science Project</title>
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet">
  <link rel="stylesheet" href="style.css">
</head>

<body>
  <div class="container">
    <div class="nav-wrapper">
      <div class="left-side">
        <div class="nav-link-wrapper active-nav-link">
          <a href="DS_project.html">Data Science Project</a>
        </div>
      </div>
    </div>

    <div class="banner-wrapper">
        <div class="profile-image-wrapper">
          <img src="images/ds-01.jpeg" alt="" width="1000">
        </div>
    </div>

    <div class="text-wrapper">
        <div class="text-title">Classify proteins into one of 15 classes</div>
        <div class="text-subtitles">Read Protein Dataset</div>
        <div class="text-body">
            <ul>
                <li>Import the dataset with pd.read_csv</li>
                <li>Set the last column to be the label column, columns before to be the features columns</li>
                <li>The label column ranges from 0-14, and there are 2961 features columns before preprocessing</li>
                <li>Split the dataset to 25% test data, 75% train data</li>
                <br>
            </ul>
        </div>

        <div class="text-subtitles">Preprocess Data</div>
        <div class="text-body">
            <ul>
                <li>Drop all columns with only NaNs</li>
                <li>Fill NaN with medium for numeric columns</li>
                <li>Fill NaN with the most frequent value for categorical columns</li>
                <li>Encode yes/no to 1/0 for columns(Class, Complex, Phenotype, Motif)</li>
                <li>Apply one-hot encoding for columns(Essential, Interaction, Chromosome)</li>
                <li>Drop columns which contain only one distinct value</li>
            </ul>
            <p>Now the dataset only consist with 1/0, by printing the first few columns, the dataset looks like</p>
            <img src="images/ds-02.png" alt="" width="800"> 
            <p>By plotting the histogram of the label, we see that the dataset is highly imbalanced.</p>
            <img src="images/ds-03.png" alt="" width="600"> 
            <p>Thus our next step is to resample the dataset.</p>
            <br>
        </div>

        <div class="text-subtitles">Handle Imbalanced Data</div>
        <div class="text-body">
            <ul>
                <li>Since class 11, 12, 14 only have 1, 2, 1 data, we use oversample to generate data for class 11, 12, 14</li>
                <li>We use SMOTE to generate new and synthetic data for class 0-10</li>
            </ul>
            <p>By plotting the label table, we see each class has 271 data. Now we are good to apply algorithms. </p>>
            <img src="images/ds-04.png" alt="" height="300"> 
            <p>Here I will apply </p>
            <p>
               <ul>
                   <li>Random Forest Classifier</li>
                   <li>K-Nearest Neighbor Classifier</li>
                   <li>Support Vector Machine</li>
                   <li>Gradient Boosting Machines</li>
                   <li>XGBoost</li>
                   <li>Stacking Classifier</li>
                </ul>
            </p>
            <br>
        </div>

        <div class="text-subtitles">Random Forest Classifer</div>
        <div class="text-body">
            <p>Hyperparameters: </p>
            <ul>
                <li>Number of trees in random forest</li>
                <li>Number of features to consider at every split</li>
                <li>Maximum number of levels in tree</li>
                <li>Minimum number of samples required to split a node</li>
                <li>Minimum number of samples required at each leaf node</li>
            </ul>
            <p>I corss validated the hyperparameter grid and obtained the following parameter values that maximizes the Testing accuracy score</p>
            <pre>
                <code>
{'n_estimators': 700, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 60}
Training dataset accuracy score:  0.9728590250329381
Testing dataset accuracy score:  0.7222222222222222
                </code>
            </pre>
        </div>

        <div class="text-subtitles">K-Nearest Neighbor Classifier</div>
        <div class="text-body">
            <p>Hyperparameters: </p>
            <ul>
                <li>Weights</li>
                <li>Number of neighbors</li>
                <li>Power parameter for the Minkowski metric</li>
            </ul>
            <p>After Grid Search Cross Validation, the tuned hyperparameter values and accuracy scores are</p>
            <pre>
                <code>
{'n_neighbors': 2, 'p': 1, 'weights': 'distance'}
Training dataset accuracy score:  0.9994729907773386
Testing dataset accuracy score:  0.6759259259259259
                </code>
            </pre>
        </div>

        <div class="text-subtitles">Support Vector Machine</div>
        <div class="text-body">
            <p>Hyperparameters: </p>
            <ul>
                <li>C</li>
                <li>gamma</li>
            </ul>
            <p>After Grid Search Cross Validation, the tuned hyperparameter values and accuracy scores are</p>
            <pre>
                <code>
{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}
Training dataset accuracy score:  0.9989459815546772
Testing dataset accuracy score:  0.7175925925925926
                </code>
            </pre>
        </div>

        <div class="text-subtitles">Gradient Boosting Machines</div>
        <div class="text-body">
            <p>Hyperparameters: </p>
            <ul>
                <li>Learning Rate</li>
                <li>Number of estimators</li>
            </ul>
            <p>Gradient Boosting algorithm took so long to run that I have to terminate the program. <br>
               The reason can be found <a href="https://stats.stackexchange.com/questions/144897/gradientboostclassifiersklearn-takes-very-long-time-to-train">here</a></p>
            <br>
        </div>

        <div class="text-subtitles">XGBoost</div>
        <div class="text-body">
            <p>Hyperparameters: </p>
            <ul>
                <li>Learning Rate</li>
                <li>Maximum tree depth</li>
                <li>Gamma</li>
                <li>Lambda</li>
            </ul>
            <p>After Grid Search Cross Validation, the tuned hyperparameter values and accuracy scores are</p>
            <pre><code>
{'max_depth': 6, 'lambda': 0.5, 'gamma': 5, 'eta': 0.5}
Training dataset accuracy score:  0.9675889328063241
Testing dataset accuracy score:  0.6944444444444444
            </code></pre>
        </div>

        <div class="text-subtitles">Stacking</div>
        <div class="text-body"> 
            <p>
            The StackingClassifier is defined by first choosing 5 base models, then defining the Logistic Regression metal model
            to combine the predictions obtained from the base models using 5-fold cross-validation.
            Here I chose 5 base models to be Logistic Regression, Naive Bayes, Random Forest, KNN, and SVC.
            After evaluating the models using cross-validation, the box plot of the accuracy scores looks like
            </p>
            <img src="images/ds-05.png" alt="" width="600">
            <p>
            From the boxplot, we see Logistic Regression performed the best on the data. The next one is Random Forest Classifier, as we
            found previously. We see StackingClassfier performed weirdly here. Some records showed a pretty high prediction, while 
            some are low. I think it might due to the high imbalance of the dataset I used to train the model on. 
            <br>
            </p>
        </div>

        <div class="text-subtitles">Conclusion</div>
        <div class="text-body">
            <p>We see that Random Forest algorithm gave the highest prediction accuracy score. <br>
               This is probably because Decision Tree performs well on imbalanced data. <br>
               I also have high expectation on XGBoost algorithm. Since XGBoost also takes super long to run, <br>
               I tuned hyperparameters only based on 2 iterations. I believe the score will be higher with more trials on hyperparameters. <br>
               I also conluded that preprocessing the data is the most important step in improving the performance of machine learning models. <br>
               I did this project twice, and by carefully preprocessed the data in the second time, <br>
               the prediction score on testing data is 4-6% higher than the first trial. <br>
            </p>

        </div>

        <div class="text-subtitles">Project Code can be found</div>
        <div class="text-body">
            <p>
            <a href="https://github.com/leslie-lu-hub/Protein-Classifier/blob/main/protein_pred.py">here</a>
            <br>
            </p>
        </div>

        <div class="text-subtitles">Reference</div>
        <div class="text-body">
            <ul>
            <li><a href="https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114">https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114</a></li>
            <li><a href="https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18">https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18</a></li>
            <li><a href="https://towardsdatascience.com/understanding-random-forest-58381e0602d2">https://towardsdatascience.com/understanding-random-forest-58381e0602d2</a></li>
            <li><a href="https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74">https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74</a></li>
            <li><a href="https://medium.com/@ODSC/optimizing-hyperparameters-for-random-forest-algorithms-in-scikit-learn-d60b7aa07ead">https://medium.com/@ODSC/optimizing-hyperparameters-for-random-forest-algorithms-in-scikit-learn-d60b7aa07ead</a></li>
            <li><a href="https://medium.com/analytics-vidhya/evaluating-a-random-forest-model-9d165595ad56">https://medium.com/analytics-vidhya/evaluating-a-random-forest-model-9d165595ad56</a></li>
            <li><a href="https://towardsdatascience.com/knn-in-python-835643e2fb53">https://towardsdatascience.com/knn-in-python-835643e2fb53</a></li>
            <li><a href="https://www.datasciencelearner.com/gradient-boosting-hyperparameters-tuning/">https://www.datasciencelearner.com/gradient-boosting-hyperparameters-tuning/</a></li>
            <li><a href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/">https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/</a></li>
            <li><a href="https://xgboost.readthedocs.io/en/latest/parameter.html">https://xgboost.readthedocs.io/en/latest/parameter.html</a></li>
            <li><a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/</a></li>
            </ul>
        </div>  
    </div>

</script>

</html>